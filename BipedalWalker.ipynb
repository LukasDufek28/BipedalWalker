{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Importy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import TD3\n",
        "from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor, DummyVecEnv\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.noise import NormalActionNoise\n",
        "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "benchmark = \"BipedalWalker-v3\"\n",
        "model_ = TD3\n",
        "max_stepov_na_epizodu = 1000\n",
        "num_envs = 6\n",
        "\n",
        "# Custom environment wrapper (optional reward shaping)\n",
        "class CustomBipedalWalker(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "\n",
        "# Function to create monitored env\n",
        "def make_env():\n",
        "    env = gym.make(benchmark, max_episode_steps=max_stepov_na_epizodu)\n",
        "    env = CustomBipedalWalker(env)\n",
        "    env = Monitor(env)  # Required for episode reward tracking\n",
        "    return env\n",
        "\n",
        "# Vectorized environments with monitoring\n",
        "vec_env = SubprocVecEnv([make_env for _ in range(num_envs)])\n",
        "vec_env = VecMonitor(vec_env)\n",
        "\n",
        "# Action noise for TD3\n",
        "action_noise = NormalActionNoise(\n",
        "    mean=np.zeros(vec_env.action_space.shape),\n",
        "    sigma=0.1 * np.ones(vec_env.action_space.shape)\n",
        ")\n",
        "\n",
        "# Custom callback for logging average reward\n",
        "class AvgRewardCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0):\n",
        "        super().__init__(verbose)\n",
        "        self.episode_rewards = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if \"infos\" in self.locals:\n",
        "            for info in self.locals[\"infos\"]:\n",
        "                if \"episode\" in info:\n",
        "                    reward = info[\"episode\"][\"r\"]\n",
        "                    self.episode_rewards.append(reward)\n",
        "                    if len(self.episode_rewards) >= 100:\n",
        "                        avg_reward = sum(self.episode_rewards[-100:]) / 100\n",
        "                        self.logger.record(\"custom/avg_reward_100ep\", avg_reward)\n",
        "        return True\n",
        "\n",
        "# Eval environment\n",
        "eval_env = Monitor(gym.make(benchmark, max_episode_steps=max_stepov_na_epizodu))\n",
        "\n",
        "def make_eval_env():\n",
        "    env = gym.make(benchmark, max_episode_steps=max_stepov_na_epizodu)\n",
        "    env = CustomBipedalWalker(env)\n",
        "    env = Monitor(env)\n",
        "    return env\n",
        "\n",
        "eval_env = DummyVecEnv([make_eval_env])\n",
        "eval_env = VecMonitor(eval_env)\n",
        "\n",
        "\n",
        "# Eval callback\n",
        "eval_callback = EvalCallback(\n",
        "    eval_env,\n",
        "    best_model_save_path=\"./models/\",\n",
        "    log_path=\"./log/\",\n",
        "    eval_freq=5000,\n",
        "    deterministic=True,\n",
        "    render=False\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define model\n",
        "model = model_(\n",
        "    'MlpPolicy',\n",
        "    vec_env,\n",
        "    verbose=1,\n",
        "    device=\"cuda\",\n",
        "    action_noise=action_noise,\n",
        "    tensorboard_log=\"./log/\" + model_.__name__ + \"_\" + benchmark,\n",
        "    batch_size=256,\n",
        "    learning_rate=0.0003,\n",
        "    buffer_size = 1_000_000,\n",
        "    gamma=0.99,\n",
        "    learning_starts=10000,\n",
        "    policy_delay=2,\n",
        "    target_policy_noise=0.2,\n",
        "    target_noise_clip=0.5,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train with callbacks\n",
        "model.learn(\n",
        "    total_timesteps=2_000_000,\n",
        "    callback=[eval_callback, AvgRewardCallback()],\n",
        "    progress_bar=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(model_.__name__ + \"_\" + benchmark)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "benchmark = \"BipedalWalkerHardcore-v3\"\n",
        "model_ = TD3\n",
        "max_stepov_na_epizodu = 1000\n",
        "num_envs = 6\n",
        "\n",
        "# Custom environment wrapper (optional reward shaping)\n",
        "class CustomBipedalWalker(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "\n",
        "# Function to create monitored env\n",
        "def make_env():\n",
        "    env = gym.make(benchmark, max_episode_steps=max_stepov_na_epizodu)\n",
        "    env = CustomBipedalWalker(env)\n",
        "    env = Monitor(env)  # Required for episode reward tracking\n",
        "    return env\n",
        "\n",
        "# Vectorized environments with monitoring\n",
        "vec_env = SubprocVecEnv([make_env for _ in range(num_envs)])\n",
        "vec_env = VecMonitor(vec_env)\n",
        "\n",
        "# Action noise for TD3\n",
        "action_noise = NormalActionNoise(\n",
        "    mean=np.zeros(vec_env.action_space.shape),\n",
        "    sigma=0.1 * np.ones(vec_env.action_space.shape)\n",
        ")\n",
        "\n",
        "# Custom callback for logging average reward\n",
        "class AvgRewardCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0):\n",
        "        super().__init__(verbose)\n",
        "        self.episode_rewards = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if \"infos\" in self.locals:\n",
        "            for info in self.locals[\"infos\"]:\n",
        "                if \"episode\" in info:\n",
        "                    reward = info[\"episode\"][\"r\"]\n",
        "                    self.episode_rewards.append(reward)\n",
        "                    if len(self.episode_rewards) >= 100:\n",
        "                        avg_reward = sum(self.episode_rewards[-100:]) / 100\n",
        "                        self.logger.record(\"custom/avg_reward_100ep\", avg_reward)\n",
        "        return True\n",
        "\n",
        "# Eval environment\n",
        "eval_env = Monitor(gym.make(benchmark, max_episode_steps=max_stepov_na_epizodu))\n",
        "\n",
        "def make_eval_env():\n",
        "    env = gym.make(benchmark, max_episode_steps=max_stepov_na_epizodu)\n",
        "    env = CustomBipedalWalker(env)\n",
        "    env = Monitor(env)\n",
        "    return env\n",
        "\n",
        "eval_env = DummyVecEnv([make_eval_env])\n",
        "eval_env = VecMonitor(eval_env)\n",
        "\n",
        "\n",
        "# Eval callback\n",
        "eval_callback = EvalCallback(\n",
        "    eval_env,\n",
        "    best_model_save_path=\"./models/\",\n",
        "    log_path=\"./log/\",\n",
        "    eval_freq=5000,\n",
        "    deterministic=True,\n",
        "    render=False\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "model = TD3.load(\"TD3_BipedalWalker-v3\", env=vec_env)\n",
        "\n",
        "# Train with callbacks\n",
        "model.learn(\n",
        "    total_timesteps=1_000_000,\n",
        "    callback=[eval_callback, AvgRewardCallback()],\n",
        "    progress_bar=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(model_.__name__ + \"_\" + benchmark + \"_hardcore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = model_.load(model_.__name__ + \"_\" + benchmark + \"_hardcore\") # Načítanie modelu\n",
        "env = gym.make(benchmark, render_mode=\"human\", max_episode_steps=max_stepov_na_epizodu)\n",
        "\n",
        "\n",
        "# Spustenie evaluacie\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10, deterministic=True)\n",
        "print(mean_reward, std_reward)\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
